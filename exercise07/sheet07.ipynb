{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Kernels for Genes Sequences\n",
    "\n",
    "In this first exercise, various *degree kernels* such as the weighted degree kernel (WDK) will be implemented. We will use Scikit-Learn (http://scikit-learn.org/) for training SVMs. The focus of this exercise is therefore on the computation of the kernels.\n",
    "\n",
    "We consider a problem of binary classification of genes sequences. The training and test data is available in the folder `splices-data`. The following code reads the gene sequences data and stores it in numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Xtrain = np.array([np.array(list(l)) for l in open('splice-data/splice-train-data.txt','r')])\n",
    "Xtest  = np.array([np.array(list(l)) for l in open('splice-data/splice-test-data.txt','r')])\n",
    "Ttrain = np.array([int(l) for l in open('splice-data/splice-train-label.txt','r')])\n",
    "Ttest  = np.array([int(l) for l in open('splice-data/splice-test-label.txt','r')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree Kernels (20 P)\n",
    "\n",
    "We consider the degree kernel of degree $d$ applying to two genes sequences $x$ and $x'$ and defined as:\n",
    "\n",
    "$$\n",
    "k_d(x,x') = \\sum_{l=1}^{L-d+1} \\boldsymbol{1}_{u_{l,d}(x) = u_{l,d}(x')}\n",
    "$$\n",
    "\n",
    "where $l$ iterates over the whole genes sequence, $u_{l,d}(x)$ is a subsequence of $x$ starting at position $l$ and of length $d$, and $\\boldsymbol{1}_{\\{\\}}$ is an indicator variable for the equality test given as argument. Given a training set and test set of genes sequences, implement a function that *efficiently* computes the kernel matrices for a certain degree $d \\in \\{1,2,3,4\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "def getdegreekernels(Xtrain,Xtest,degree):\n",
    "    \n",
    "    Ktrain = np.zeros((len(Xtrain),len(Xtrain)))\n",
    "    Ktest = np.zeros((len(Xtest),len(Xtrain)))\n",
    "    \n",
    "    for i in range(len(Xtrain)):\n",
    "        x = Xtrain[i]\n",
    "        m = Xtrain == x\n",
    "        c = convolve2d(m, [[1]*degree])\n",
    "        sums = np.sum(c == degree, axis=1)\n",
    "        Ktrain[i] = sums\n",
    "        \n",
    "    for i in range(len(Xtest)):\n",
    "        x = Xtest[i]\n",
    "        m = Xtrain == x\n",
    "        c = convolve2d(m, [[1]*degree])\n",
    "        sums = np.sum(c == degree, axis=1)\n",
    "        Ktest[i] = sums    \n",
    "    \n",
    "    assert(Ktrain.shape==(len(Xtrain),len(Xtrain)) and Ktest.shape==(len(Xtest),len(Xtrain)))\n",
    "    return Ktrain, Ktest\n",
    "\n",
    "Ktrain, Ktest = getdegreekernels(Xtrain, Xtest, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below calls the function you implemented for various degrees `d`, trains SVMs based on these kernels, and measures the prediction accuracy. It can be expected to run in less than 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree: 1   training accuracy: 0.994   test accuracy: 0.916\n",
      "degree: 2   training accuracy: 1.000   test accuracy: 0.934\n",
      "degree: 3   training accuracy: 1.000   test accuracy: 0.964\n",
      "degree: 4   training accuracy: 1.000   test accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "Ktrains, Ktests = [None]*4,[None]*4\n",
    "\n",
    "for i in range(4):\n",
    "    Ktrains[i], Ktests[i] = getdegreekernels(Xtrain, Xtest, i+1)\n",
    "    mysvm = svm.SVC(kernel='precomputed').fit(Ktrains[i], Ttrain)\n",
    "    Ytrain = mysvm.predict(Ktrains[i])\n",
    "    Ytest = mysvm.predict(Ktests[i])\n",
    "    print('degree: %d   training accuracy: %.3f   test accuracy: %.3f'% \\\n",
    "          (i+1,(Ytrain==Ttrain).mean(), (Ytest==Ttest).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Degree Kernel (10 P)\n",
    "\n",
    "We now consider a weighted degree kernel with uniform weights:\n",
    "\n",
    "$$\n",
    "k(x,x') = \\sum_{d=1}^4 k_d(x,x')\n",
    "$$\n",
    "\n",
    "where $k_d(x,x')$ is the kernel with degree $d$ that was implemented in the previous section. *Construct* the kernel matrices for the weighted degree kernel and *compute* the training and test accuracy of an SVM trained with this new kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000   test accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "mysvm = svm.SVC(kernel='precomputed').fit(sum(Ktrains), Ttrain)\n",
    "Ytrain = mysvm.predict(sum(Ktrains))\n",
    "Ytest = mysvm.predict(sum(Ktests))\n",
    "print('accuracy: %.3f   test accuracy: %.3f'% ((Ytrain==Ttrain).mean(), (Ytest==Ttest).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Kernels for Text\n",
    "\n",
    "Structured kernels can also be used for classifying text data. In this exercise, we consider the classification of a subset of the 20-newsgroups data (available at http://qwone.com/~jason/20Newsgroups/). A subset of this data composed only of texts of class `comp.graphics` and `sci.med` is given in the folder `newsgroup-data`. The first class is assigned label `-1` and the second class is assigned label `+1`. Furthermore, the beginning and the end of the newsgroup messages are removed as they typically contain information that makes the classification problem trivial. Like for the genes sequences dataset, data files are composed of multiple rows, where each row corresponds to one example. The code below extracts the fifth message of the training set and displays its 500 first characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hat is, >>center and radius, exactly fitting those points?  I know how\n",
      "to do it >>for a circle (from 3 points), but do not immediately see a\n",
      ">>straightforward way to do it in 3-D.  I have checked some >>geometry\n",
      "books, Graphics Gems, and Farin, but am still at a loss? >>Please have\n",
      "mercy on me and provide the solution?   > >Wouldn't this require a\n",
      "hyper-sphere.  In 3-space, 4 points over specifies >a sphere as far as\n",
      "I can see.  Unless that is you can prove that a point >exists in\n",
      "3-space that  [...]\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "text = list(open('newsgroup-data/newsgroup-train-data.txt','r'))[4]\n",
    "print(textwrap.fill(text[:500]+' [...]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bag-Of-Words (10 P)\n",
    "\n",
    "A convenient way of representing text data is as bag-of-words: a set composed of all the words occuring in the document. For the purpose of this exercise, we formally define a word as an isolated sequence of at least three consecutive alphabetical characters. Furthermore, a set of `stopwords` containing mostly uninformative words such as prepositions or conjunctions that should be excluded from the bag-of-word representation is provided in the file `stopwords.txt`. Create a function `text2bow(text)` that converts a text into a bag of words following the just described specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text2bow(text):\n",
    "    bow = set()\n",
    "    stopwords = list(open('stopwords.txt','r'))\n",
    "    words = re.findall(\"[a-zA-Z]{3,}\", text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords and re.match(r\"^[a-zA-Z]{3,}$\", word):\n",
    "            bow.add(word)        \n",
    "    return bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your bag-of-words implementation can be tested for the same text shown above by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['all', 'hyper', 'consider', 'over', 'steve', 'distant', 'yes',\n",
      "'still', 'its', 'gems', 'intersection', 'choose', 'circle', 'hat',\n",
      "'wouldn', 'non', 'straightforward', 'far', 'possibly', 'cannot',\n",
      "'equidistant', 'know', 'they', 'not', 'immediately', 'one', 'loss',\n",
      "'solution', 'either', 'through', 'specifies', 'right', 'exists',\n",
      "'some', 'relative', 'see', 'books', 'radius', 'are', 'unless',\n",
      "'happen', 'best', 'subject', 'will', 'abc', 'for', 'space',\n",
      "'pictures', 'normal', 'please', 'coplaner', 'find', 'does', 'correct',\n",
      "'perpendicular', 'quite', 'desired', 'let', 'checked',\n",
      "'circumference', 'geometry', 'could', 'passing', 'otherwise',\n",
      "'diameter', 'point', 'numerically', 'three', 'lies', 'equi',\n",
      "'necessarily', 'from', 'prove', 'there', 'bisectors', 'least',\n",
      "'their', 'farin', 'call', 'way', 'define', 'that', 'exactly', 'but',\n",
      "'failure', 'bisector', 'angles', 'graphics', 'line', 'sorry', 'those',\n",
      "'must', 'mercy', 'algorithm', 'this', 'require', 'collinear', 'can',\n",
      "'error', 'meet', 'and', 'coincident', 'then', 'defined', 'them',\n",
      "'surface', 'sphere', 'say', 'have', 'close', 'need', 'any', 'lie',\n",
      "'containing', 'since', 'provide', 'check', 'how', 'fitting', 'take',\n",
      "'which', 'you', 'infinity', 'centre', 'may', 'wrong', 'plane', 'two',\n",
      "'such', 'center', 'four', 'well', 'points', 'very', 'the', 'normally',\n",
      "'fact'])\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(str(text2bow(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Bag-Of-Words Kernels (15 P)\n",
    "\n",
    "In the following, your task is to implement a simple kernel over bag-of-words. The kernel between two bag-of-words $\\mathcal{X}$ and $\\mathcal{Y}$ is defined as\n",
    "\n",
    "$$\n",
    "k(\\mathcal{X},\\mathcal{Y}) = \\sum_{w \\in \\mathcal{L}} 1_{w \\in \\mathcal{X} \\wedge w \\in \\mathcal{Y}}\n",
    "$$\n",
    "\n",
    "where $1_{w \\in \\mathcal{X} \\wedge w \\in \\mathcal{Y}}$ is an indicator function testing membership to both bags of words. The language $\\mathcal{L}$ (set of all existing words) is typically unknown and very large. However, it is computationally equivalent to reduce the language $\\mathcal{L}$ to the union $\\mathcal{X} \\cup \\mathcal{Y}$ of the two considered bag-of-words. Thus, we can rewrite the kernel as:\n",
    "\n",
    "$$\n",
    "k(\\mathcal{X},\\mathcal{Y}) = \\sum_{w \\in (\\mathcal{X} \\cup \\mathcal{Y})} 1_{w \\in \\mathcal{X} \\wedge w \\in \\mathcal{Y}}\n",
    "$$\n",
    "\n",
    "*Create* a kernel method that implements this kernel function in a *naive* way. Your naive implementation will then be compared to an optimized one. The naive implementation can be summarized as follows:\n",
    "\n",
    "* Iterate over all possible words $w$ in $\\mathcal{X} \\cup \\mathcal{Y}$.\n",
    "* At each iteration test membership of $w$ to $\\mathcal{X}$ and $\\mathcal{Y}$.\n",
    "* If both memberships are satisfied, increment the kernel score by 1. If not, leave it to its current value.\n",
    "\n",
    "*Remark:* To test the membership of $w$ to $\\mathcal{X}$ and $\\mathcal{Y}$, do *not* use the operator \"`in`\" in Python, as it makes use of special data structures behind the scenes. Instead, iterate over all elements of $\\mathcal{X}$ and $\\mathcal{Y}$ using a `for` loop, and test membership using \"`==`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_naive(bow1, bow2):\n",
    "    bows = set()\n",
    "    bows.update(bow1)\n",
    "    bows.update(bow2)\n",
    "    sum_ = 0\n",
    "    for w in bows:\n",
    "        exists1 = False\n",
    "        for x in bow1:\n",
    "            if x == w:\n",
    "                exists1 = True\n",
    "                break\n",
    "        if not exists1:\n",
    "            continue\n",
    "        for y in bow2:\n",
    "            if y == w:\n",
    "                sum_ += 1\n",
    "                break\n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `analyze_worstcase_performance(text2bow,kernel)` in `utils.py` computes the worst-case performance (i.e. when applied to the two longest texts in the dataset) of a specific kernel. Run the code below to test the performance of your implementation of the naive kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel score: 827.000 , computation time: 0.528\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "utils.analyze_worstcase_performance(text2bow, kernel_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline implementation can be greatly accelerated (by a factor more than 100) by sorting the words in the bag-of-words in alphabetic order, and making use of the new sorted structure in the kernel implementation. In the code below, the sorted list associated to `bow1` is called `sbow1`. *Implement* a function `kernel_sorted(sbow1,sbow2)` that takes as input two lists of words (sorted in alphabetic order) and computes the kernel value in a more efficient manner. Like for the naive implementation, do *not* use the Python operator \"`in`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_sorted(sbow1,sbow2):\n",
    "    sum_ = 0\n",
    "    \n",
    "    i = j = 0\n",
    "    while i != len(sbow1) and j != len(sbow2):\n",
    "        if sbow1[i] < sbow2[j]:\n",
    "            i += 1\n",
    "        elif sbow1[i] == sbow2[j]:\n",
    "            sum_ += 1\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    \n",
    "    return sum_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimized kernel can be tested for worst case performance by running the code below. Here, we define an additional method `text2sbow(text)` for computing the sorted bag-of-words. Verify that the kernel score remains the same as with the naive implementation. The computation time is expected to drop drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel score: 827.000 , computation time: 0.001\n"
     ]
    }
   ],
   "source": [
    "def text2sbow(text): return sorted(list(text2bow(text)))\n",
    "\n",
    "import utils\n",
    "utils.analyze_worstcase_performance(text2sbow,kernel_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Documents with a Kernel SVM (15 P)\n",
    "\n",
    "The kernel function between two text documents can be used to build a SVM-based text classifier. Here, we would like to disriminate between the two classes `comp.graphics` and `sci.med` present in the dataset. The code below reads the whole dataset and stores input (mapped to sorted bag-of-words) and labels in the appropriate data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 106)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = map(text2sbow, open('newsgroup-data/newsgroup-train-data.txt','r'))\n",
    "Xtest  = map(text2sbow, open('newsgroup-data/newsgroup-test-data.txt','r'))\n",
    "Ttrain = np.array(map(int, open('newsgroup-data/newsgroup-train-label.txt','r')))\n",
    "Ttest  = np.array(map(int, open('newsgroup-data/newsgroup-test-label.txt','r')))\n",
    "print(len(Xtrain),len(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, one needs to build the kernel matrices between pairs of training examples and between training and test examples. After evaluating whether building such matrices is computationally feasible given the performance of your optimized bag-of-words kernel implementation, write the function `build_kernels(Xtrain,Xtest)` for constructing these matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kernels(Xtrain, Xtest):\n",
    "    \n",
    "    Ktrain = np.zeros((len(Xtrain),len(Xtrain)))\n",
    "    Ktest = np.zeros((len(Xtest),len(Xtrain)))\n",
    "    \n",
    "    for i in range(len(Xtrain)):\n",
    "        for j in range(len(Xtrain)):\n",
    "            Ktrain[i, j] = kernel_sorted(Xtrain[i], Xtrain[j])\n",
    "        \n",
    "    for i in range(len(Xtest)):\n",
    "        for j in range(len(Xtrain)):\n",
    "            Ktrain[i, j] = kernel_sorted(Xtest[i], Xtrain[j])\n",
    "    \n",
    "    assert(Ktrain.shape==(len(Xtrain), len(Xtrain)) and Ktest.shape==(len(Xtest), len(Xtrain)))\n",
    "    return Ktrain, Ktest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These kernel matrices along with the vector of training labels `Ttrain` can be used to train an SVM in the same way as in the previous exercise on genes sequences classification. Write a function that trains an SVM (using scikit-learn with default parameters) and computes the predictions on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_prediction(Ktrain,Ttrain,Ktest):\n",
    "    \n",
    "    mysvm = svm.SVC(kernel='precomputed').fit(Ktrain, Ttrain)\n",
    "    Ytrain = mysvm.predict(Ktrain)\n",
    "    Ytest = mysvm.predict(Ktest)\n",
    "    print('training accuracy: %.3f   test accuracy: %.3f'% \\\n",
    "          ((Ytrain==Ttrain).mean(), (Ytest==Ttest).mean()))\n",
    "    \n",
    "    assert(Ytrain.shape==Ttrain.shape and Ytest.shape==Ttest.shape)\n",
    "    return Ytrain,Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the functions that you have implemented for classifying the texts can be tested by measuring the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ktrain, Ktest = build_kernels(Xtrain, Xtest)\n",
    "Ytrain, Ytest = get_svm_prediction(Ktrain, Ttrain, Ktest)\n",
    "\n",
    "print('training accuracy: %.3f   test accuracy: %.3f'% ((Ytrain==Ttrain).mean(), (Ytest==Ttest).mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
