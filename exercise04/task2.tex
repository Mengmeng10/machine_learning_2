\section*{Task 2}

\subsection*{a}

To get the same optimization problem, the only thing that has to be adapted is the last term, since instead of $||\vec s_i||_1$ we have $||\vec r_i||^2$.
The connection between $\vec r$ and $\vec s$ is given as $\vec s_i = \vec g(\vec r_i)$, so the condition that needs to be fulfilled is
\begin{align*}
\forall^N_{i=1}: \quad ||\vec r_i||^2 \overset{!}{=} ||\vec g(\vec r_i)||_1 \quad ,
\end{align*}
where the $\vec s_i$ only have positive components.

This condition can be met by choosing the components of $\vec{g}(\vec{r}_i)$ as
\begin{align*}
g_j(\vec{r}_i) = r_{ij}^2
\end{align*}

On the one hand, the square ensures that all components are positive, and on the other hand we get
\begin{align*}
||\vec s_i||_1 = \sum^h_{j=1} |s_{ij}| = \sum^h_{j=1} |g_j(\vec{r}_i)|
= \sum^h_{j=1} |r_{ij}^2| = \sum^h_{j=1} r_{ij}^2 = ||\vec r_i||^2 \quad .
\end{align*}

\subsection*{b}

If $g(r)$ is differentiable, we can apply backpropagation as before to find the
$r$'s. If we pick $g(r)$ such that it depends on $x$ i.e. $g(x)$, then  we can also use it as
a good initialization for the sources.